---
layout: post
title: 一位人工智能风险专家认为，政府应该采取行动打击错误信息。受邀请进行翻译。

excerpt_separator: <!--more-->
---


<!--more-->

# 一位AI风险专家认为，政府应该采取行动来打击错误信息。| 邀请撰写


# 一位AI风险专家认为，政府应该采取行动来对抗误导性信息。| 邀请致辞


# An AI-risk expert thinks governments should act to combat disinformation | By Invitation

一位人工智能风险专家认为，政府应当采取行动来打击错误信息。| 凭邀请撰写


# An AI-risk expert thinks governments should act to combat disinformation | By Invitation

![image](https://images.weserv.nl/?url=www.economist.com/img/b/1280/720/90/media-assets/image/20240210_BID001.jpg)

<div></div><p><span>O</span><small>NE DAY</small> last November, Olaf Scholz addressed the German people with an unexpected announcement: his government was to request the Federal Constitutional Court to ban the “fascist” Alternative für Deutschland, a far-right political party. A video containing the German leader’s message appeared on a website created specifically for that purpose.</p>

去年11月某日，奥拉夫·朔尔茨（Olaf Scholz）这位德国领导人向国民发表了一则出乎意料的声明：他的政府将请求联邦宪法法院禁止“法西斯主义”政党 Alternative für Deutschland（AfD）。一段包含德国领导人信息的视频出现在一个专门为此次目的创建的网站上。




去年11月的一天，奥拉夫·朔尔茨面对德国人民做出了一个意想不到的宣布：他的政府将请求联邦宪法法院禁止“法西斯主义”政党 Alternative für Deutschland（AfD）。一段包含这位德国领导人的信息视频出现在专门为这个目的创建的网站上。


<p>Only it wasn’t the real Mr Scholz. A German group of guerrilla artists had used artificial intelligence (<small>AI</small>) to create a “deepfake”: an image or video generated by machine-learning software. Just a few hours after the clip went live, a government spokesman condemned the “manipulative” nature of such videos and their potential to “stir up uncertainty”.</p>

这段视频上线仅几个小时后，政府发言人就谴责了此类视频“具有操纵性”的本质及其可能“制造不确定性的潜力”。




这段视频上线仅仅数小时后，政府发言人就谴责了此类视频“具有操纵性”的本质，以及它们可能“煽动不确定性”的风险。


<div><div><div id="econ-1"></div></div></div><p>Britain’s National Cyber Security Centre recently raised similar concerns that deepfakes could compromise democratic discourse and upcoming elections through targeted disinformation. As politicians fret about the trend, ordinary voters are growing more worried, too. According to a poll by Ipsos last year, a majority of people in numerous countries, including America, Britain and France, believe that <small>AI</small> will make disinformation worse.</p>

英国国家网络安全中心（National Cyber Security Centre，NCSC）最近也提出了类似的担忧，即深度伪造可能会通过有针对性的虚假信息，破坏民主讨论和即将到来的选举。

当政治家们对这一趋势表示忧虑时，普通选民也越来越感到不安。根据去年Ipsos的一项民调，包括美国、英国和法国在内的多个国家中的大多数人都认为人工智能将使虚假信息问题更加严重。




英国国家网络安全中心最近也表达了类似的担忧，即深度伪造可能通过有针对性的误导信息，破坏民主对话和即将举行的选举。当政治人物对这一趋势感到忧虑时，普通选民也越来越担心。根据去年Ipsos进行的一项民意调查，包括美国、英国和法国在内的多个国家的多数人认为人工智能会使误导性信息更加严重。


<p>Yet there is actually a lot of uncertainty about how real this danger is. Despite the use of advanced <small>AI</small>, the deepfake of Mr Scholz is easily given away as fake by out-of-sync lip movement and an unnatural voice. The same is true for the majority of deepfakes currently circulating on social media. Are fears of <small>AI</small>-generated disinformation exaggerated?</p>

然而，关于这种威胁的真实性到底有多少不确定性，人们其实是感到困惑的。尽管使用了先进的人工智能技术，但Scholz先生的深度伪造视频很容易因为嘴唇动作不协调和声音自然度不足而被识破是假的。社交媒体上目前流传的大多数深度伪造视频也是如此。对于人工智能生成的假信息引发的恐慌是否被夸大了？




然而，关于这种危险的真实程度实际上存在很多不确定性。尽管使用了先进的AI技术，但对施罗兹先生的深度伪造很容易通过不匹配的唇部动作和不自然的声音被识破为假象。社交媒体上目前流传的大多数深度伪造也同样如此。对于由人工智能生成的虚假信息的担忧是否被夸大了？


<p>Some experts point to studies from before the rise of generative <small>AI</small> that show that disinformation campaigns are generally of limited success. For example, Chris Bail, a sociologist at Duke University, and colleagues looked at a concerted Russian disinformation campaign on Twitter in 2017 and concluded that it largely failed to sow political division among Americans. People’s general reluctance to change their political views in response to <i>any</i> piece of information, though often a curse, might in this case be a blessing.</p>

一些专家引用了生成式AI兴起前的研究，这些研究表明，虚假信息运动通常成功率有限。例如，杜克大学的社会学家克里斯·贝尔和他的同事们研究了2017年俄罗斯在Twitter上发起的一场有组织的虚假信息运动，他们得出结论认为，这场运动在美国人中制造政治分歧的主要目标并未取得成功。

人们对任何信息都普遍不愿意改变自己的政治观点，虽然这种现象常常带来问题。但在这个案例中，人们对于信息的保守态度可能反而成了一个有利的因素。




一些专家引用了生成式AI兴起之前的研究，这些研究表明，虚假信息运动通常成功率有限。例如，杜克大学社会学家克里斯·贝尔（Chris Bail）和他的同事们研究了2017年俄罗斯在Twitter上发起的有组织的虚假信息运动，他们得出结论认为，这场运动在煽动美国政治分歧方面并未取得显著成功。

人们普遍不愿意因为任何信息而改变自己的政治观点，尽管这常常是一种诅咒。但在这种情况下，它可能是件好事。


<p><small>AI</small> is already changing the disinformation landscape, but the impact is different for static media like text and images than for audio and video. In the case of the former, <small>AI</small> doesn’t really boost the quality of disinformation. Studies suggest that <small>AI</small>-generated text is at best slightly more convincing than human-written text, and often no more so; Photoshop made sophisticated fake images possible long before generative <small>AI</small> was available. The change is, rather, quantitative: <small>AI</small> makes it much easier to produce and distribute disinformation at scale.</p>

人工智能已经在改变信息误导的格局，但对静态媒体（如文本和图片）的影响与音频和视频不同。对于前者，AI并没有真正提高信息误导的质量。研究表明，由AI生成的文本最多只是比人类编写的文本稍微更具说服力而已，而且常常并非如此；早在生成式AI可用之前，Photoshop就使得制作复杂的假图像变得可能。变化更多的是定量上的：AI使大规模生产和分发信息误导变得更加容易。




人工智能已经在改变假信息的传播环境，但其对静态媒体如文本和图片的影响与音频和视频不同。对于前者来说，AI并没有真正提高假信息的质量。研究表明，AI生成的文本最多只比人类撰写的文本稍微更有说服力，而且常常没有差别；早在生成式AI可用之前，Photoshop就已经使得制作复杂的假图像变得可能。改变更多的是数量级上的：AI大大简化了在大规模范围内生产并分发假信息的过程。


<div><div><div id="econ-2"></div></div></div><p>Large language models (<small>LLM</small>s) such as Chat<small>GPT</small> generate high-quality text at practically no cost; tools like Midjourney allow even amateurs to create realistic-looking images with simple prompts. <small>LLM</small>s can help write computer code to automate the spread of disinformation. They have been used to generate “alternative news” websites that mix truth and lies but are hard to shut down because of concerns over censorship. <small>AI</small>-operated social-media profiles, so-called “social bots”, can manipulate algorithms to push certain types of content. In the future, such bots might turn into full-blown online personas that enter into relationships with users and target them with subtle personalised messages.</p>

大型语言模型（LLMs）如ChatGPT几乎无需成本就能生成高质量的文本。像Midjourney这样的工具甚至让业余人士也能通过简单的提示创建看起来逼真的图像。

LLMs能帮助编写计算机代码，自动化虚假信息的传播过程。它们已经被用来生成混合真相和谎言的“替代新闻”网站，由于担心审查，这些网站很难关闭。

由人工智能操作的社交媒体账号，即所谓的“社交机器人”，可以通过操纵算法来推送给用户特定类型的内容。未来，这样的机器人可能会发展成为完整的在线存在，与用户建立关系，并以微妙的个性化信息为目标用户。




大型语言模型（LLMs）如ChatGPT以近乎零成本生成高质量文本；类似Midjourney的工具甚至让业余者也能通过简单的提示创造出逼真的图像。

LLMs有助于编写计算机代码来自动传播假信息。它们已被用于生成混杂真相和谎言的“替代新闻”网站，但因为担心审查而难以关闭。

人工智能运营的社交媒体账号，所谓的“社交机器人”，可以通过操纵算法来推广特定类型的内容。未来，这样的机器人可能会发展成完整的在线角色，与用户建立关系，并通过微妙的个性化信息为目标用户定向。

综上所述，LLMs和相关技术正在以前所未有的方式改变我们创建、传播和消费内容的方式。


<p>Perhaps none of this would be enough to sway mass opinion. But that doesn’t mean there’s no danger. Studies have found that merely knowing about the spread of disinformation lowers public trust in the media, even the most reliable sources. That, in turn, increases the “liar’s dividend”: the relative ease with which politicians can denounce compromising evidence as fake. </p>

也许这些都不足以扭转大众舆论。但这并不意味着不存在危险。研究发现，仅仅是了解到虚假信息的传播，就足以降低公众对媒体的信任——哪怕是最可靠的来源。而这又进一步加剧了“说谎者得益”现象：政客们可以轻易地否认具有妥协性质的证据，声称它是假的。




也许这些都不足以动摇大众舆论。但这并不意味着不存在危险。研究发现，仅仅是了解假信息的传播，就会降低公众对媒体的信任，哪怕是最可靠的来源也不例外。这反过来又加剧了“骗子的好处”：政治家可以轻易地否认令人妥协的证据是伪造的，这个过程变得更加容易。


<p>The biggest concerns are reserved for dynamic media. On top of a quantitative boost, <small>AI</small> adds a whole new qualitative dimension to audio and video. Previously, it just wasn’t possible to create realistic fake videos of public figures, except perhaps for Hollywood studios. We’re moving closer to a world where anyone can craft such footage on their laptop.</p>

最大的担忧留给了动态媒体。除了数量上的增长，AI还在音频和视频上带来了全新的、质的飞跃。以前，除了好莱坞工作室可能能做到，创造出逼真的公众人物虚假视频几乎是不可能的。我们现在正朝着一个世界前进，在那里任何人都可以在笔记本电脑上制作出这样的片段。




最大的担忧集中在动态媒体上。除了数量上的增长，AI还为音频和视频带来了全新的质性维度。以前，除了好莱坞工作室，几乎不可能创造出逼真的公众人物假视频。我们正在接近一个世界，在那里任何人都可以在笔记本电脑上制作这样的镜头。


<p>Last August Brendan Nyhan of Dartmouth College said in <i>The Economist</i>: “We still have not one convincing case of a deepfake making any difference whatsoever in politics”. We might have seen such a case just a few weeks later, in Slovakia. Two days before the national election in September 2023, an audio deepfake began circulating on social media. The widely shared clip appeared to be of Michal Šimečka, leader of the Progressive Slovakia party, discussing plans to rig the election. Because the clip gained traction during a 48-hour news moratorium before the vote, it was hard to debunk in time. After a tight race, Mr Šimečka’s party lost by five percentage points to that of Robert Fico, a pro-Russian populist.</p>

原文：
去年8月，达特茅斯学院的布伦丹·尼汉在《经济学人》上表示：“我们至今仍未看到一起令人信服的深度造假案例，该案例无论大小，都对政治产生了影响。”几周后，也就是几周之后，在斯洛伐克可能就出现了这样的一起案件。

全文翻译：
去年8月，达特茅斯学院的布伦丹·尼汉在《经济学人》上说：“我们至今还未遇到一起确凿无疑的深度伪造案例，它无论大小，都能对政治产生影响。”几周后，在斯洛伐克可能就出现了这样的一起案例。在全国选举前两天，一段音频深度造假开始在社交媒体上流传。广泛分享的剪辑似乎显示了米哈伊尔·希梅奇卡，进步斯洛伐克党领袖，正在讨论操纵选举的计划。由于这段视频在投票前48小时新闻禁令期间获得了关注，因此很难及时揭露真相。经过激烈的竞争，希梅奇卡的政党最终以5个百分点的差距输给了罗伯特·费科，一名亲俄民粹主义者。
assistant.




原文翻译成中文：

去年8月，达特茅斯学院的布雷登·尼汉(Brendan Nyhan)在《经济学人》(The Economist)上说：“我们至今还没有一起令人信服的情况，深伪造像在政治上产生了任何区别。”仅仅几周后，这样的情况可能就在斯洛伐克发生了。

2023年9月的全国选举前两天，一段音频深伪造像开始在社交媒体上传播。这段广泛分享的片段似乎显示了米哈尔·希梅奇卡(Michal Šimečka)正在讨论，他是进步斯洛伐克党(Progressive Slovakia Party)的领导人，计划操纵选举。因为这段视频在投票前48小时内新闻静默期期间获得了关注，很难及时揭露其真伪。经过一场紧咬不放的比赛后，希梅奇卡先生的政党最终以5个百分点输给了罗伯特·菲科（Robert Fico），后者是一名亲俄民粹主义者。


<p>Experts remain divided on the impact of <small>AI</small>-generated disinformation on democracy and elections, and it may be some time before it is clear. But politicians can’t afford to wait for the dust to settle; <small>AI</small> is moving too fast for a “wait and see” approach. They must decide now how much risk to public discourse they are willing to accept. Fortunately, there are a number of interventions that can be made to boost its resilience.</p>

专家们对于人工智能生成的虚假信息对民主和选举的影响意见不一，可能需要一段时间才能明朗。但政界人士不能坐等尘埃落定；人工智能的发展速度超出了“观望”策略的适应范围。他们必须现在就决定愿意接受多少公共讨论的风险。

幸运的是，有许多干预措施可以采取，以增强其抗压能力。




专家们对于人工智能生成的假信息对民主和选举产生的影响仍然意见分歧，可能还需要一段时间才能清晰。但政治家们不能等待尘埃落定后再行动；人工智能的发展速度太快，“等着看”已经来不及了。他们必须现在就决定愿意接受多少公共对话的风险。

幸运的是，有许多干预措施可以采取来增强其抗风险能力。




<p>First, <small>AI</small> companies should be incentivised to develop watermarking and detection tools, used to distinguish <small>AI</small>-generated from authentic content, that actually work (current tools can too easily be evaded, or “washed out”). They could be given a choice: develop models that refuse to generate certain types of content, such as realistic videos of public figures, or find a way to reliably watermark their output (or make it otherwise detectable).</p>

首先，应激励AI公司开发有效的水印和检测工具。这些工具要能够区分人工智能生成的真实内容与原创的内容，并且必须切实有效（目前的工具往往容易规避或“冲洗”掉）。

公司可以选择其中一条道路：开发模型，使其拒绝生成特定类型的内容，例如名人公开场合的逼真视频；或者找到一种可靠的方法，在它们的产品输出中添加水印（或其他方式使其易于检测）。




首先，应激励AI公司开发有效的水印和检测工具。这些工具应该能够区分由AI生成的与真实内容不同的内容，并且实际有效（目前的工具容易被规避或“洗掉”）。

公司可以选择发展拒绝生成某些类型内容的模型，例如公众人物的逼真视频，或者找到一种可靠的方法在它们的输出上打水印（或其他方式使其可检测）。


<p>Second, such technical fixes should be flanked by tried-and-tested “prebunking” interventions. These can be media campaigns, short videos or games that educate people about the goals and strategies of disinformation campaigns and enhance public resistance to fake news.</p>

其次，这类技术性补丁应配合经过验证的“预先反虚假信息”干预措施。这些可以是媒体运动、简短的视频或游戏，它们教育人们了解虚假信息运动的目标和策略，并增强公众对假新闻的抵抗力。




第二，技术性补救措施应与经过验证的“预反虚假信息”干预手段相辅相成。这些干预手段可以是媒体活动、简短的视频或游戏，它们旨在教育公众了解虚假信息运动的目标和策略，并增强公众对假新闻的抵抗力。


<div><div><div id="econ-3"></div></div></div><p>Third, we need robust monitoring regimes and third-party evaluations to keep future model capabilities in check. For example, governments must know when models become able to autonomously wage manipulation campaigns via online personas. Without such oversight, policymakers won’t be able to react in time if things go really awry.</p>

第三，我们需要强大的监控制度和第三方评估，以确保未来模型的能力得到控制。例如，政府必须知道何时模型能够自主地通过在线身份发起操纵活动。如果没有这样的监督，政策制定者将无法及时做出反应，如果情况变得非常糟糕的话。




第三，我们需要强大的监控体系和第三方评估，以确保未来模型的能力不会失控。例如，政府必须知道何时模型能够通过在线身份自主发起操纵战。如果没有这样的监管，政策制定者将无法及时应对真正严重的局面。


<p>Before the age of generative <small>AI</small>, Photoshop didn’t make it impossible to tell true from false, just as Wikipedia didn’t make everyone intellectually lazy. Yet <small>AI</small> brings risks of a different order of magnitude. Since the technology is moving at a daunting pace, countermeasures might come too late unless policymakers act now. That this is a “year of elections”, with more than half the world’s population living in countries that will send citizens to the polls, makes it even more urgent to act. Taking a soft stance on <small>AI</small>-generated disinformation is not worth the risk.<span>■</span></p>

在生成性人工智能时代到来之前，Photoshop并没有让分辨真假变得不可能，就像维基百科并没有让所有人都变得智力懒惰一样。然而，人工智能带来了不同数量级的风险。由于技术发展速度惊人，除非政策制定者立即行动，否则可能为时已晚，无法及时采取应对措施。

鉴于今年是选举之年，全球超过一半的人口居住在即将举行选举的国家，因此现在比以往任何时候都更加迫切需要立即采取行动。

对生成性人工智能带来的虚假信息持软弱立场并不可取。■




在生成性人工智能的时代之前，Photoshop并没有让分辨真伪变得不可能，正如维基百科并没有使所有人都变得智力懒惰。然而，人工智能带来了不同数量级的风险。由于这项技术的进展速度令人震惊，除非政策制定者现在就行动，否则可能为时已晚采取对策。

考虑到今年是选举之年，世界上超过一半的人口居住在即将举行选民投票的国家中，这使得立即采取行动变得更加紧迫。

对人工智能生成的虚假信息持软弱立场的风险并不值得。■


<p><i>Philip Fox is an analyst at the <small>KIRA</small> Center for <small>AI </small>Risks &amp; Impacts, an independent think-tank in Berlin.</i></p>

菲利普·福克斯是柏林独立智库航空风险与影响中心（KIRA Center for AIRisks & Impacts）的一名分析师。




菲利普·福克斯是柏林独立智囊机构空气风险与影响中心（KIRA Center for AIRisks & Impacts）的一名分析师。


