---
layout: post
title: 一位人工智能风险专家认为，政府应当采取行动来打击错误信息
---

# An  thinks governments should act to combat disinformation | By Invitation

<!--more-->

## 一位人工智能风险专家认为，政府应当采取行动来打击错误信息

![image](https://images.weserv.nl/?url=www.economist.com/img/b/1280/720/90/media-assets/image/20240210_BID001.jpg)

<div></div><p><span>O</span><small>NE DAY</small> last November, Olaf Scholz addressed the German people with an unexpected announcement: his government was to request the Federal Constitutional Court to ban the “fascist” Alternative für Deutschland, a far-right political party. A video containing the German leader’s message appeared on a website created specifically for that purpose.</p>

去年11月的一天，奥拉夫·朔尔茨面对德国人民做出了一个意想不到的宣布：他的政府将请求联邦宪法法院禁止“法西斯主义”政党 Alternative für Deutschland（AfD）。一段包含这位德国领导人的信息视频出现在专门为这个目的创建的网站上。


<p>Only it wasn’t the real Mr Scholz. A German group of guerrilla artists had used artificial intelligence (<small>AI</small>) to create a “deepfake”: an image or video generated by machine-learning software. Just a few hours after the clip went live, a government spokesman condemned the “manipulative” nature of such videos and their potential to “stir up uncertainty”.</p>

这段视频上线仅仅数小时后，政府发言人就谴责了此类视频“具有操纵性”的本质，以及它们可能“煽动不确定性”的风险。


<div><div><div id="econ-1"></div></div></div><p>Britain’s National Cyber Security Centre recently raised similar concerns that deepfakes could compromise democratic discourse and upcoming elections through targeted disinformation. As politicians fret about the trend, ordinary voters are growing more worried, too. According to a poll by Ipsos last year, a majority of people in numerous countries, including America, Britain and France, believe that <small>AI</small> will make disinformation worse.</p>

英国国家网络安全中心最近也表达了类似的担忧，即深度伪造可能通过有针对性的误导信息，破坏民主对话和即将举行的选举。当政治人物对这一趋势感到忧虑时，普通选民也越来越担心。根据去年Ipsos进行的一项民意调查，包括美国、英国和法国在内的多个国家的多数人认为人工智能会使误导性信息更加严重。


<p>Yet there is actually a lot of uncertainty about how real this danger is. Despite the use of advanced <small>AI</small>, the deepfake of Mr Scholz is easily given away as fake by out-of-sync lip movement and an unnatural voice. The same is true for the majority of deepfakes currently circulating on social media. Are fears of <small>AI</small>-generated disinformation exaggerated?</p>

然而，关于这种危险的真实程度实际上存在很多不确定性。尽管使用了先进的AI技术，但对施罗兹先生的深度伪造很容易通过不匹配的唇部动作和不自然的声音被识破为假象。社交媒体上目前流传的大多数深度伪造也同样如此。对于由人工智能生成的虚假信息的担忧是否被夸大了？


<p>Some experts point to studies from before the rise of generative <small>AI</small> that show that disinformation campaigns are generally of limited success. For example, Chris Bail, a sociologist at Duke University, and colleagues looked at a concerted Russian disinformation campaign on Twitter in 2017 and concluded that it largely failed to sow political division among Americans. People’s general reluctance to change their political views in response to <i>any</i> piece of information, though often a curse, might in this case be a blessing.</p>

一些专家引用了生成式AI兴起之前的研究，这些研究表明，虚假信息运动通常成功率有限。例如，杜克大学社会学家克里斯·贝尔（Chris Bail）和他的同事们研究了2017年俄罗斯在Twitter上发起的有组织的虚假信息运动，他们得出结论认为，这场运动在煽动美国政治分歧方面并未取得显著成功。

人们普遍不愿意因为任何信息而改变自己的政治观点，尽管这常常是一种诅咒。但在这种情况下，它可能是件好事。


<p><small>AI</small> is already changing the disinformation landscape, but the impact is different for static media like text and images than for audio and video. In the case of the former, <small>AI</small> doesn’t really boost the quality of disinformation. Studies suggest that <small>AI</small>-generated text is at best slightly more convincing than human-written text, and often no more so; Photoshop made sophisticated fake images possible long before generative <small>AI</small> was available. The change is, rather, quantitative: <small>AI</small> makes it much easier to produce and distribute disinformation at scale.</p>

人工智能已经在改变假信息的传播环境，但其对静态媒体如文本和图片的影响与音频和视频不同。对于前者来说，AI并没有真正提高假信息的质量。研究表明，AI生成的文本最多只比人类撰写的文本稍微更有说服力，而且常常没有差别；早在生成式AI可用之前，Photoshop就已经使得制作复杂的假图像变得可能。改变更多的是数量级上的：AI大大简化了在大规模范围内生产并分发假信息的过程。


<div><div><div id="econ-2"></div></div></div><p>Large language models (<small>LLM</small>s) such as Chat<small>GPT</small> generate high-quality text at practically no cost; tools like Midjourney allow even amateurs to create realistic-looking images with simple prompts. <small>LLM</small>s can help write computer code to automate the spread of disinformation. They have been used to generate “alternative news” websites that mix truth and lies but are hard to shut down because of concerns over censorship. <small>AI</small>-operated social-media profiles, so-called “social bots”, can manipulate algorithms to push certain types of content. In the future, such bots might turn into full-blown online personas that enter into relationships with users and target them with subtle personalised messages.</p>

大型语言模型（LLMs）如ChatGPT以近乎零成本生成高质量文本；类似Midjourney的工具甚至让业余者也能通过简单的提示创造出逼真的图像。

LLMs有助于编写计算机代码来自动传播假信息。它们已被用于生成混杂真相和谎言的“替代新闻”网站，但因为担心审查而难以关闭。

人工智能运营的社交媒体账号，所谓的“社交机器人”，可以通过操纵算法来推广特定类型的内容。未来，这样的机器人可能会发展成完整的在线角色，与用户建立关系，并通过微妙的个性化信息为目标用户定向。

综上所述，LLMs和相关技术正在以前所未有的方式改变我们创建、传播和消费内容的方式。


<p>Perhaps none of this would be enough to sway mass opinion. But that doesn’t mean there’s no danger. Studies have found that merely knowing about the spread of disinformation lowers public trust in the media, even the most reliable sources. That, in turn, increases the “liar’s dividend”: the relative ease with which politicians can denounce compromising evidence as fake. </p>

也许这些都不足以动摇大众舆论。但这并不意味着不存在危险。研究发现，仅仅是了解假信息的传播，就会降低公众对媒体的信任，哪怕是最可靠的来源也不例外。这反过来又加剧了“骗子的好处”：政治家可以轻易地否认令人妥协的证据是伪造的，这个过程变得更加容易。


<p>The biggest concerns are reserved for dynamic media. On top of a quantitative boost, <small>AI</small> adds a whole new qualitative dimension to audio and video. Previously, it just wasn’t possible to create realistic fake videos of public figures, except perhaps for Hollywood studios. We’re moving closer to a world where anyone can craft such footage on their laptop.</p>

最大的担忧集中在动态媒体上。除了数量上的增长，AI还为音频和视频带来了全新的质性维度。以前，除了好莱坞工作室，几乎不可能创造出逼真的公众人物假视频。我们正在接近一个世界，在那里任何人都可以在笔记本电脑上制作这样的镜头。


<p>Last August Brendan Nyhan of Dartmouth College said in <i>The Economist</i>: “We still have not one convincing case of a deepfake making any difference whatsoever in politics”. We might have seen such a case just a few weeks later, in Slovakia. Two days before the national election in September 2023, an audio deepfake began circulating on social media. The widely shared clip appeared to be of Michal Šimečka, leader of the Progressive Slovakia party, discussing plans to rig the election. Because the clip gained traction during a 48-hour news moratorium before the vote, it was hard to debunk in time. After a tight race, Mr Šimečka’s party lost by five percentage points to that of Robert Fico, a pro-Russian populist.</p>

原文翻译成中文：

去年8月，达特茅斯学院的布雷登·尼汉(Brendan Nyhan)在《经济学人》(The Economist)上说：“我们至今还没有一起令人信服的情况，深伪造像在政治上产生了任何区别。”仅仅几周后，这样的情况可能就在斯洛伐克发生了。

2023年9月的全国选举前两天，一段音频深伪造像开始在社交媒体上传播。这段广泛分享的片段似乎显示了米哈尔·希梅奇卡(Michal Šimečka)正在讨论，他是进步斯洛伐克党(Progressive Slovakia Party)的领导人，计划操纵选举。因为这段视频在投票前48小时内新闻静默期期间获得了关注，很难及时揭露其真伪。经过一场紧咬不放的比赛后，希梅奇卡先生的政党最终以5个百分点输给了罗伯特·菲科（Robert Fico），后者是一名亲俄民粹主义者。


<p>Experts remain divided on the impact of <small>AI</small>-generated disinformation on democracy and elections, and it may be some time before it is clear. But politicians can’t afford to wait for the dust to settle; <small>AI</small> is moving too fast for a “wait and see” approach. They must decide now how much risk to public discourse they are willing to accept. Fortunately, there are a number of interventions that can be made to boost its resilience.</p>

专家们对于人工智能生成的假信息对民主和选举产生的影响仍然意见分歧，可能还需要一段时间才能清晰。但政治家们不能等待尘埃落定后再行动；人工智能的发展速度太快，“等着看”已经来不及了。他们必须现在就决定愿意接受多少公共对话的风险。

幸运的是，有许多干预措施可以采取来增强其抗风险能力。




<p>First, <small>AI</small> companies should be incentivised to develop watermarking and detection tools, used to distinguish <small>AI</small>-generated from authentic content, that actually work (current tools can too easily be evaded, or “washed out”). They could be given a choice: develop models that refuse to generate certain types of content, such as realistic videos of public figures, or find a way to reliably watermark their output (or make it otherwise detectable).</p>

首先，应激励AI公司开发有效的水印和检测工具。这些工具应该能够区分由AI生成的与真实内容不同的内容，并且实际有效（目前的工具容易被规避或“洗掉”）。

公司可以选择发展拒绝生成某些类型内容的模型，例如公众人物的逼真视频，或者找到一种可靠的方法在它们的输出上打水印（或其他方式使其可检测）。


<p>Second, such technical fixes should be flanked by tried-and-tested “prebunking” interventions. These can be media campaigns, short videos or games that educate people about the goals and strategies of disinformation campaigns and enhance public resistance to fake news.</p>

第二，技术性补救措施应与经过验证的“预反虚假信息”干预手段相辅相成。这些干预手段可以是媒体活动、简短的视频或游戏，它们旨在教育公众了解虚假信息运动的目标和策略，并增强公众对假新闻的抵抗力。


<div><div><div id="econ-3"></div></div></div><p>Third, we need robust monitoring regimes and third-party evaluations to keep future model capabilities in check. For example, governments must know when models become able to autonomously wage manipulation campaigns via online personas. Without such oversight, policymakers won’t be able to react in time if things go really awry.</p>

第三，我们需要强大的监控体系和第三方评估，以确保未来模型的能力不会失控。例如，政府必须知道何时模型能够通过在线身份自主发起操纵战。如果没有这样的监管，政策制定者将无法及时应对真正严重的局面。


<p>Before the age of generative <small>AI</small>, Photoshop didn’t make it impossible to tell true from false, just as Wikipedia didn’t make everyone intellectually lazy. Yet <small>AI</small> brings risks of a different order of magnitude. Since the technology is moving at a daunting pace, countermeasures might come too late unless policymakers act now. That this is a “year of elections”, with more than half the world’s population living in countries that will send citizens to the polls, makes it even more urgent to act. Taking a soft stance on <small>AI</small>-generated disinformation is not worth the risk.<span>■</span></p>

在生成性人工智能的时代之前，Photoshop并没有让分辨真伪变得不可能，正如维基百科并没有使所有人都变得智力懒惰。然而，人工智能带来了不同数量级的风险。由于这项技术的进展速度令人震惊，除非政策制定者现在就行动，否则可能为时已晚采取对策。

考虑到今年是选举之年，世界上超过一半的人口居住在即将举行选民投票的国家中，这使得立即采取行动变得更加紧迫。

对人工智能生成的虚假信息持软弱立场的风险并不值得。■


<p><i>Philip Fox is an analyst at the <small>KIRA</small> Center for <small>AI </small>Risks &amp; Impacts, an independent think-tank in Berlin.</i></p>

菲利普·福克斯是柏林独立智囊机构空气风险与影响中心（KIRA Center for AIRisks & Impacts）的一名分析师。



